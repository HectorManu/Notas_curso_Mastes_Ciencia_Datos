{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Libresía y csv \n",
    "como siempre cargamso el CSV que vmoas a utilizar. Así mismo, vmoas a cargar las librerías principales que utilizaremos en esta sección. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "data = pd.read_csv(filename, names = names)\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Validación cruzada\n",
    "La validación cruzada es un proceso en el que se realizan ***K*** particiones o **folds** de la base de datos y con ellos se realizan ***K*** evaluaciones diferentes, de tal forma que todos los casos por lo menos una vez se muestran en el conjunto de test. Básicamente en la evaluazión *i* , partición *i* son los casos de test y el resto so nlos casos de entrenamiento. Finalmente, se realiza una media de los resutltaods obtenidos en las diferentes evaluaziones. En la siguiente imagen se ve un ejemplo de esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.34 % (5.61%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10)\n",
    "modelo = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "restultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(f\"Accuracy: {restultados.mean()*100.0:,.2f} % ({restultados.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Validación cruzada repetida\n",
    "Una extensió d eesta técnica de entrenamiento/validación es el poder repetis varias veces el proceso de dividir los dados Ik-fold, En este caso, la precisión final del modelo se toma como la media dle número de repeticiones. <br>Al igual que el caso anterior podemos observar que nos imforma tanto la meida como la desviasión estándar de la media de rendimeinto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'n_repeats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[0;32m    824\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Documentos\\proyectos\\Master_ciencia_datos\\05_Metodos_remuestreo.ipynb Celda 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m kfold \u001b[39m=\u001b[39m RepeatedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m modelo \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m,max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m restultados \u001b[39m=\u001b[39m cross_val_score(modelo, X, Y, cv\u001b[39m=\u001b[39;49mRepeatedKFold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00mrestultados\u001b[39m.\u001b[39mmean()\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m,.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m % (\u001b[39m\u001b[39m{\u001b[39;00mrestultados\u001b[39m.\u001b[39mstd()\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m,.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    507\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 509\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    510\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    511\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    512\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    513\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    514\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    515\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    516\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    517\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    518\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    519\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    520\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m         clone(estimator),\n\u001b[0;32m    270\u001b[0m         X,\n\u001b[0;32m    271\u001b[0m         y,\n\u001b[0;32m    272\u001b[0m         scorers,\n\u001b[0;32m    273\u001b[0m         train,\n\u001b[0;32m    274\u001b[0m         test,\n\u001b[0;32m    275\u001b[0m         verbose,\n\u001b[0;32m    276\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    277\u001b[0m         fit_params,\n\u001b[0;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    830\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    831\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 833\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m         clone(estimator),\n\u001b[0;32m    270\u001b[0m         X,\n\u001b[0;32m    271\u001b[0m         y,\n\u001b[0;32m    272\u001b[0m         scorers,\n\u001b[0;32m    273\u001b[0m         train,\n\u001b[0;32m    274\u001b[0m         test,\n\u001b[0;32m    275\u001b[0m         verbose,\n\u001b[0;32m    276\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m         fit_params,\n\u001b[0;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[0;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1406\u001b[0m, in \u001b[0;36m_RepeatedSplits.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1383\u001b[0m     \u001b[39m\"\"\"Generates indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m \n\u001b[0;32m   1385\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[39m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1406\u001b[0m     n_repeats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_repeats\n\u001b[0;32m   1407\u001b[0m     rng \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m   1409\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_repeats):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'n_repeats'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "num_repeated = 5\n",
    "kfold = RepeatedKFold(n_splits=2)\n",
    "modelo = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "restultados = cross_val_score(modelo, X, Y, cv=RepeatedKFold)\n",
    "print(f\"Accuracy: {restultados.mean()*100.0:,.2f} % ({restultados.std()*100.0:,.2f}%)\")\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# y = np.array([0, 0, 1, 1])\n",
    "# rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
    "# for train_index, test_index in rkf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Validazión cruzada dejando uno fuera\n",
    "\n",
    "En validación cruzada dejando uno fuera (Leave One Out Cross Validation, LOOCV), se omite una instancia de datos y se construeye un modelo en todas las demás instancias de datos en el conjunto de entrenamiento, repitiéndose este proceos para todas las instancias de datos. El resultado es  una gran cantidad de medidas de rendimeinto que se pueden reseumir en un esfuerzo por proporcionar una estimación más razonable del *Accuarazy* de su modelo. <br>Puede ver en la desviación estándar que la puntuación tiene máz varianza que los resultados de validación cruzada *k-fold* descritos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy: 77.47% (41.78%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "resultados = cross_val_score(model,X,Y,cv=loocv)\n",
    "print(f\"Accuaracy: {resultados.mean()*100.0:,.2f}% ({resultados.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 División en porcentaje train/test\n",
    "<br>La división de datos implica la partición de los datos en un conjunto de datos de entrenamiento explícito utilizado para preparar el modelo y un conjunto de datos de validación invisible (se dice validación invisible ya que aunque se conozca el atribuot clase el modleo lo omitirá para unavez realiada la predicción compararlo para ver si ha acertado) que se utiliza para evaluar el rendimiento del modelo en datos no etiquetados. <br>En el ejemplo a continuación, dividimos el conjunto de datos de los inidios Pima en divisiones de 67%/33% para entrenamiento y prueba y evaluar el Accuracy de un modelo Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy: 78.74%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "resultados= model.score(X_test, Y_test)\n",
    "print(f\"Accuaracy: {resultados*100.0:,.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 División train/test repetidos aleatoriamente\n",
    "<br> Otra variazión en la validación cruzada de k-fold es crear una división aleatoria de los datos como la división de train/test descrita anteriormente, pero repetir el proceso de división y evaluazión dle algoritmo varias veces, como la validación cruzada. El siguiente ejemplo divide los datos en una división d train/test del 67%755% y repite el proceso 10 veces.<br><br>Podemos ver que en este caso la distribución de la media de desempeño está a la par con k-fold cross validation pero reducimoe la varianza considerablemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy: 76.61% (2.32%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "n_splits = 10\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size, random_state=seed)\n",
    "\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size,random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs',max_iter = 1000)\n",
    "resultados = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f\"Accuaracy: {resultados.mean()*100.0:,.2f}% ({resultados.std()*100.0:,.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Ciencia_Datos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "743777b310bb13500d41c4574cfbba7071203b1e5d57bcccb6c8c7a477518f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
