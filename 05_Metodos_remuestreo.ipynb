{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Libresía y csv \n",
    "como siempre cargamso el CSV que vmoas a utilizar. Así mismo, vmoas a cargar las librerías principales que utilizaremos en esta sección. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "data = pd.read_csv(filename, names = names)\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Validación cruzada\n",
    "La validación cruzada es un proceso en el que se realizan ***K*** particiones o **folds** de la base de datos y con ellos se realizan ***K*** evaluaciones diferentes, de tal forma que todos los casos por lo menos una vez se muestran en el conjunto de test. Básicamente en la evaluazión *i* , partición *i* son los casos de test y el resto so nlos casos de entrenamiento. Finalmente, se realiza una media de los resutltaods obtenidos en las diferentes evaluaziones. En la siguiente imagen se ve un ejemplo de esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.34 % (5.61%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10)\n",
    "modelo = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "restultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(f\"Accuracy: {restultados.mean()*100.0:,.2f} % ({restultados.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Validación cruzada repetida\n",
    "Una extensió d eesta técnica de entrenamiento/validación es el poder repetis varias veces el proceso de dividir los dados Ik-fold, En este caso, la precisión final del modelo se toma como la media dle número de repeticiones. <br>Al igual que el caso anterior podemos observar que nos imforma tanto la meida como la desviasión estándar de la media de rendimeinto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'n_repeats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[0;32m    824\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Documentos\\proyectos\\Master_ciencia_datos\\05_Metodos_remuestreo.ipynb Celda 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m kfold \u001b[39m=\u001b[39m RepeatedKFold( n_repeats\u001b[39m=\u001b[39mnum_repeated)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m modelo \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m,max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m restultados \u001b[39m=\u001b[39m cross_val_score(modelo, X, Y, cv\u001b[39m=\u001b[39;49mRepeatedKFold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Documentos/proyectos/Master_ciencia_datos/05_Metodos_remuestreo.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00mrestultados\u001b[39m.\u001b[39mmean()\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m,.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m % (\u001b[39m\u001b[39m{\u001b[39;00mrestultados\u001b[39m.\u001b[39mstd()\u001b[39m*\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m,.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    507\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 509\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    510\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    511\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    512\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    513\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    514\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    515\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    516\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    517\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    518\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    519\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    520\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m         clone(estimator),\n\u001b[0;32m    270\u001b[0m         X,\n\u001b[0;32m    271\u001b[0m         y,\n\u001b[0;32m    272\u001b[0m         scorers,\n\u001b[0;32m    273\u001b[0m         train,\n\u001b[0;32m    274\u001b[0m         test,\n\u001b[0;32m    275\u001b[0m         verbose,\n\u001b[0;32m    276\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    277\u001b[0m         fit_params,\n\u001b[0;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\joblib\\parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    830\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    831\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 833\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m         clone(estimator),\n\u001b[0;32m    270\u001b[0m         X,\n\u001b[0;32m    271\u001b[0m         y,\n\u001b[0;32m    272\u001b[0m         scorers,\n\u001b[0;32m    273\u001b[0m         train,\n\u001b[0;32m    274\u001b[0m         test,\n\u001b[0;32m    275\u001b[0m         verbose,\n\u001b[0;32m    276\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m         fit_params,\n\u001b[0;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[0;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Aplicaciones\\anaconda3\\envs\\Ciencia_Datos\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1406\u001b[0m, in \u001b[0;36m_RepeatedSplits.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1383\u001b[0m     \u001b[39m\"\"\"Generates indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m \n\u001b[0;32m   1385\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[39m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1406\u001b[0m     n_repeats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_repeats\n\u001b[0;32m   1407\u001b[0m     rng \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m   1409\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_repeats):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'n_repeats'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "num_repeated = 5\n",
    "kfold = RepeatedKFold(n_splits = num_folds, n_repeats=num_repeated, random_state=seed)\n",
    "modelo = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "restultados = cross_val_score(modelo, X, Y, cv=RepeatedKFold)\n",
    "print(f\"Accuracy: {restultados.mean()*100.0:,.2f} % ({restultados.std()*100.0:,.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Ciencia_Datos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "743777b310bb13500d41c4574cfbba7071203b1e5d57bcccb6c8c7a477518f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
